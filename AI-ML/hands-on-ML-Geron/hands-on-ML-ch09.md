# Chapter 9 - Unsupervised Learning

## Types of Unsupervised Problems
- Dimension Reduction
- Clustering
- Anomaly Detection
- Density Estimation

## Clustering

### Potential Usecases of Clustering
- Customer segmentation (used in recommender systems)
- Data Analysis
- Dimension Reduction: instance vector can be encoded by the affinity vector with clusters
- Anomaly (Outlier) Detection: instances that has a low affinity with all or some clusters can be defined as anomalies.
- Semi-Supervised Learning: when only a few labels are available, cluster the data first and assign the label to the clusters (or only the instances that are close enough to the centroids)
- Search engine: cluster images so people can search for similar images
- Segment a image: cluster pixles according to their color, then replacing each pixle's color with the mean of its cluster.

### K-Means
- `sklearn.cluster.KMeans(n_clusters=k)`
    - Data needs to be stadardized
- Hard v.s. Soft Clustering
    - Hard clustering: assign each instance to a single cluster
    - Soft clustering: give each instance a score per cluster
        - Types of scores
            - Distance between instance and centroid
            - Similarity (affinity) score like Gaussian Radial Basis Function
- K-Means Algorithm
    1. Initialize the k centroids randomly
    2. Label the instances by it's closest centroid
    3. Update the centroid using the latest labeled instances
    4. Repeat
- Centroid Initialization Methods
    - Initialization may impact the final optimum the algorithm reaches.
    - Methods
        - Use the pre-knowledge to define the approximate centroids
        - Run the algorithm different times with different random seeds and keep the best solution (lowest inertia)
        - K-Means++; Used by Scikit-Learn by default
            1. Choose random uniform centroids
            2. Choose a new centroids by choosing an instance with probability "square of the instance-centroid distance / sum of that of all instances". The instances that are farther away from the current centroid are more likely to be chosen as the new centroid.
            3. Repeat
- Evaluation metric
    - Inertia: mean squared distance between each instance and its closest centroid
- Improve computational efficiency
    - Accelerated K-Means
        - Used by Scikit-Lean by default
        - Accelerate by avoid unnecessary disntance caluculation
    - Min-Batch K-Means `sklearn.cluster.MiniBatchKMeans`: slightly worse outcome but faster
- Tune the number of clusters k
    - Option 1: use inertia
        - Try different k and find the "elbow" of the inertia plot (where the inertia decreasing rate slows down)
        - Inertia always goes down with larger k. So finding the k with lowest inertia is not a good strategy.
    - Option 2: use Silhouette Coefficient (better)
        - Silhouette Coefficient = (b - a) / max(a,b) for each instance
        - a is the mean distance to the other instances in the same cluster
        - b is the mean distance to the instances of the next closest cluster, defined as one that minimizes b, excluding the instance's own cluster.
        - `sklearn.metrics.silhouette_score`: mean of silhouette coefficients of all instances
        - Silhouette Diagram
            - x-axis: silhouette coefficient
            - y-axis: instances, grouped by clusters, and ordered by silhouette coefficient descendently within each cluster
            - Vertial dash line: silhouette score (mean silhouette coefficient)
            - Good cluster: many instances have coefficients larger than the mean (cross the dashed line)
- Pros and Cons of K-Means
    - Pros: fast and scalable
    - Cons: bad for clusters with varying sizes, different densities, non-spherical (like ellipsoidal) shapes

### DBSCAN
- Algorithm
    1. For each instances, count the number of instances within a epsilon `eps` distance (epsilon neighbor)
    2. If an instance has at least `min_samples` instances in the epsilon neigbor (including itself), it is called a **core instance**.
    3. All instances in the neighborhood of a core instance belong to the same cluster. A core instance's neighborhood may include other core instances. The chain of the instances are all in the same cluster.
    4. Any instance that is not a core instance or does not have one as its neighbor is considered an anomaly.
- `sklearn.cluster.DBSCAN`

### Other Clustering Algorithms
- Agglomerative Clustering
- BIRCH
- Mean-Shift
- Affinity Propagation
- Spectral clustering

## Gaussian Mixture Model

### Usage
- Density Estimation
- Clustering
- Anomaly Detection

### General Idea
- Sample of data is generated by a mixture of several Gaussian distributions
- Each instance is generated by one of the Gaussian distributions
- All instances generated by the same Gaussian distribution is a cluster
- Each cluster typically looks like an ellipsoid with different sizes, shapes, density and orientations.
- The goal is to find how many Gaussian distributions can be fit, and their parameters.
- Does not scale well

### Classical Gaussian Mixure Model
- `sklearn.mixture.GaussianMixure`
- Assuming the number of Gaussian distributions (number of clusters) are known as k.
- Parameter of each distribution that needs to fit
    - Mean vector (center of cluster)
    - Covariance matrix
    - Weight of each distribution
- Expection-Maximization (EM) Algorithm to fit the parameters of the distributions
    1. Randomly initialize the parameters of the distribution
    2. Expectation step: for each instances, calculate the probability of such instance can be generated by each distribution based on parameters of each distribution from the previous iteration. The probability that each instance are generated by each distribution is called the **responsibility**.
    3. Maximization step: the parameter of each distribution is updated based on all the instances, with each instances weighted by the probability belonging to that cluster calculated from step 2.
- New samples can be generated from the fitted model. `sample()` method
- Density of any location within the feature space can be estimated. Exponential of the output of `score_samples()` is the probability density.
- Constraints on the covariance matrix
    - Default: `full`; non constraints
    - `spherical`: all distributions must be spherical
    - `diag`: ellipsoidal distributions with axes parallel to the coordinate axes (covariance matrices must be diagonal)
    - `tied`: all distributions mush have the same ellipsoidal shape, size, and orientation (same covariance matrices)
- Use Gaussian Mixture Models to detect anomaly
    - Define a proabability threshold
    - Find a the space where the density is lower than the threshold
    - All instances within those space are considered anomalies.
    - Different thresholds can be try-and-errored.
- Hyperparamter tuning: finding the best number of distributions (clusters)
    - Using one of the theoritical information criterion: BIC (Bayesian Information Criterion) or AIC (Akaike Information Criterion)
    - The smaller the better
    - Both "IC"s penalize models that have more parameters to learn (more clusters) and reward the models that fit the data well.
    - BIC tends to select simpler models that fit the data slightly less well.
    - `aic()` and `bic()` methods

### Bayesian Gaussian Mixure Models
- `sklearn.mixure.BayesianGaussianMixure`
- Set the `n_components` to a number likely to be larger than the opitimal one
- The algorithm can assign zero weights to unnecessary clusters
- The prior distribution of distribution weights is Beta(1,alpha) `weight_concentration_prior`
- The prior distribution of the means is Gaussian
- The prior distribution of the covariance matrix is Wishart

### Other Algorithm for Anomaly Detection
- PCA
- Fast-MCD
- Isolation Forest
- Local Outlier Factor (LOF)
- One-class SVM