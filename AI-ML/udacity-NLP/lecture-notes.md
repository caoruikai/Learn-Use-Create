# Useful Resources
- [my google drive url with saved materials](https://drive.google.com/drive/u/0/folders/1A513ghIpAMj84QkKmrdQZeoz-fP4UjDa)

## Course Site & Repo
- https://classroom.udacity.com/nanodegrees/nd892-ent/dashboard/overview
- https://github.com/udacity/AIND-VUI-Lab-Voice-Data
- https://github.com/udacity/NLP-Exercises
- https://github.com/udacity/AIND-VUI-Capstone
- https://github.com/udacity/AIND-VUI-Alexa
- https://github.com/udacity/artificial-intelligence
- https://github.com/udacity/hmm-tagger
- https://github.com/udacity/AIND-NLP
- https://github.com/udacity/deep-learning

## Books
- http://aima.cs.berkeley.edu/
- https://home.cs.colorado.edu/~martin/SLP/
- https://d2l.ai/index.html

## Papers
- [PoS Tagset](https://arxiv.org/pdf/1104.2086.pdf)
- [Stats Language Models](https://www.cs.cmu.edu/~roni/papers/survey-slm-IEEE-PROC-0004.pdf)
- [LDA](https://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf)
- [RNN](https://onlinelibrary.wiley.com/doi/epdf/10.1207/s15516709cog1402_1)
- [RNN Recommender](https://arxiv.org/pdf/1511.06939.pdf)
- [LSTM](http://www.bioinf.jku.at/publications/older/2604.pdf)
- [Gradient Clipping](https://arxiv.org/abs/1211.5063)
- [Bahdanau Attention](https://arxiv.org/abs/1409.0473)
- [Luong Attention](https://arxiv.org/abs/1508.04025)
- [Xu Attention-Image-Caption 2015](http://proceedings.mlr.press/v37/xuc15.pdf)
- [Anderson Attention-Image-Caption 2018](https://arxiv.org/pdf/1707.07998.pdf)
- [Yu Attention-Video-Caption 2016](https://www.cv-foundation.org/openaccess/content_cvpr_2016/app/S19-04.pdf)
- [Yeung Video-Action-Label 2017](https://arxiv.org/pdf/1507.05738.pdf)
- [Teney-VisualQuestion-2017](https://arxiv.org/pdf/1708.02711.pdf)
- [Wu-VisualQuestionSurvey-2016](https://arxiv.org/pdf/1607.05910.pdf)
- [Wu-GoogleTranslate-2016](https://arxiv.org/pdf/1609.08144.pdf)
- [Nallapati-TextSummarySeq2Seq-2016](https://arxiv.org/pdf/1602.06023.pdf)
- [Vaswani-Transformer-2017](https://arxiv.org/abs/1706.03762)
- [Chung GRU-vs-LSTM-2014](https://arxiv.org/pdf/1412.3555v1.pdf)
- [Narang-SpeechFeatureSurvey-2015](https://www.ijcsmc.com/docs/papers/March2015/V4I3201545.pdf)
- [Graves-CTC-2006](http://www.cs.toronto.edu/~graves/icml_2006.pdf)
- [Baidu-DeepSpeech2-2015](https://arxiv.org/pdf/1512.02595v1.pdf)
- [Liu-gramCTC-2017](https://arxiv.org/pdf/1703.00096.pdf)
- [Miao-ASRDNN-2015](https://arxiv.org/pdf/1507.08240.pdf)
- [Graves-ASRbidirectLSTM-2013](https://www.cs.toronto.edu/~graves/asru_2013.pdf)
- [Zhang-ASRCNN-2017](https://arxiv.org/pdf/1701.02720.pdf)
- [Oord-wavenet-2016](https://arxiv.org/abs/1609.03499)
- [Gal-dropoutRNN-2015](https://arxiv.org/abs/1512.05287)
- [Graves-ASRRNN-2013](http://www.cs.toronto.edu/~hinton/absps/DRNN_speech.pdf)
- [Laurent-RNNBatchNormal-2015](https://arxiv.org/pdf/1510.01378.pdf)
- [Google-RawWaveform-2015](https://www.ee.columbia.edu/~ronw/pubs/interspeech2015-waveform_cldnn.pdf)


## Knowledge
- [Penn Treebank PoS](https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html)
- [Brown Corpus](https://en.wikipedia.org/wiki/Brown_Corpus)
- [Spacy: lingustic features](https://spacy.io/usage/linguistic-features)
- [Wikipedia: Forward Algorithm](https://en.wikipedia.org/wiki/Forward_algorithm)
- [Wikipedia: Viterbi Algorithm](https://en.wikipedia.org/wiki/Viterbi_algorithm)
- [Wikipedia: Vanishing Gradient](https://en.wikipedia.org/wiki/Vanishing_gradient_problem)
- [Wikipedia: Time Delay NN](https://en.wikipedia.org/wiki/Time_delay_neural_network)
- [Wikipedia: RNN](https://en.wikipedia.org/wiki/Recurrent_neural_network#Elman_networks_and_Jordan_networks)
- [Backpropagation](https://www.ics.uci.edu/~pjsadows/notes.pdf)
- [Calculus Rules](http://www.columbia.edu/itc/sipa/math/calc_rules_multivar.html)
- [Common Derivatives Integrals](https://tutorial.math.lamar.edu/pdf/Common_Derivatives_Integrals.pdf)
- [Learning Rate in Gradient Descent](http://blog.datumbox.com/tuning-the-learning-rate-in-gradient-descent/)
- [CS321n: CNN for Vision Recognition](https://cs231n.github.io/neural-networks-3/)
- [Andrej Karpathy's lecture on RNNs and LSTMs from CS231n](https://www.youtube.com/watch?v=iX5V1WpxxkY)
- [Edwin Chen's LSTM post](http://blog.echen.me/2017/05/30/exploring-lstms/)
- [Chris Olah's LSTM post](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)
- [LSTM Tutorial](https://web.archive.org/web/20190106151528/https://skymind.ai/wiki/lstm)
- [GRU](http://www.cs.toronto.edu/~guerzhoy/321/lec/W09/rnn_gated.pdf)
- [Wikipedia: Zipf's Law](https://en.wikipedia.org/wiki/Zipf%27s_law)
- [Attention is All You Need](https://www.youtube.com/watch?v=rBCqOTEfxvg)
- [Cheng-LSTMReading-2016](https://arxiv.org/abs/1601.06733)
- [Example-based_machine_translation](https://en.wikipedia.org/wiki/Example-based_machine_translation)
- [Statistical_machine_translation](https://en.wikipedia.org/wiki/Statistical_machine_translation)
- [Rule-based_machine_translation](https://en.wikipedia.org/wiki/Rule-based_machine_translation)
- [bidirectional-lstm](https://machinelearningmastery.com/develop-bidirectional-lstm-sequence-classification-python-keras/)
- [word2vec](https://jalammar.github.io/illustrated-word2vec/)
- [word2vec-skip-gram](http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/)
- [Transformer](http://jalammar.github.io/illustrated-transformer/)
- [Visualize Embed](https://www.tensorflow.org/tensorboard/tensorboard_projector_plugin)
- [Fourier Transformation](https://ibmathsresources.com/2014/08/14/fourier-transforms-the-most-important-tool-in-mathematics/)
- [Wikipedia: Sound](https://en.wikipedia.org/wiki/Sound)
- [Mitch Marcus: Speech Recognition](https://www.seas.upenn.edu/~cis391/Lectures/speech-rec.pdf)
- [Wikipedia: Fast Fourier Transformation](https://en.wikipedia.org/wiki/Fast_Fourier_transform)
- [Wikipedia: Mel scale](https://en.wikipedia.org/wiki/Mel_scale)
- [Speech Recognition Tutorial](http://www.speech.cs.cmu.edu/15-492/slides/03_mfcc.pdf)
- [MFCC Tutorial](http://practicalcryptography.com/miscellaneous/machine-learning/guide-mel-frequency-cepstral-coefficients-mfccs/)
- [Wikipedia: ARPABET](https://en.wikipedia.org/wiki/ARPABET)
- [Wikipedia: Grapheme](https://en.wikipedia.org/wiki/Grapheme)
- [Wikipedia: Phoneme](https://en.wikipedia.org/wiki/Phoneme)
- [Phonetics](https://en.wikipedia.org/wiki/Phonetics)
- [BERT](https://en.wikipedia.org/wiki/BERT_(language_model))
- [Wikipedia: Pangram](https://en.wikipedia.org/wiki/Pangram)
- [Wikipedia: Bigram](https://en.wikipedia.org/wiki/Bigram)
- [Wikipedia: Markov Property](https://en.wikipedia.org/wiki/Markov_property)
- [Wikipedia: Additive Smoothing](https://en.wikipedia.org/wiki/Additive_smoothing)
- [ASR Course](http://www.cs.cmu.edu/~bhiksha/courses/11-756.asr/spring2014/)
- [HMM in ASR](http://www.cs.cmu.edu/~bhiksha/courses/11-756.asr/spring2014/lectures/class7-8.hmm.pdf)
- [ASR Slides](http://www.cs.cmu.edu/~bhiksha/courses/11-756.asr/spring2014/lectures/class9.continuousspeech.pdf)
- [ASR Slides](http://asr.cs.cmu.edu/spring2011/class21.6apr/class21.subwordunits.pdf)
- [ASR KaiFu Lee](https://www.youtube.com/watch?v=PJ_KCTsOCrs)
- [Baidu slides DNN ASR](https://cs.stanford.edu/~acoates/ba_dls_speech2016.pdf)
- [Baidu speech DNN ASR](https://www.youtube.com/watch?v=g-sndkf7mCs)
- [Miao slides DNN ASR](http://people.csail.mit.edu/jrg/meetings/CTC-Dec07.pdf)
- [Wikipedia: Mel-frequency_cepstrum](https://en.wikipedia.org/wiki/Mel-frequency_cepstrum)
- [Spectrograms: an Introduction](https://www.youtube.com/watch?v=_FatxGN3vAM)
- [Keras: Optimizers](https://keras.io/api/optimizers/)
- [Optimizers](https://ruder.io/optimizing-gradient-descent/index.html)
- [Computational Graph](https://medium.com/tebs-lab/deep-neural-networks-as-computational-graphs-867fcaa56c9)


## Softwares
- [Pomegranate](https://pomegranate.readthedocs.io/en/latest/index.html)
- [NLTK](https://www.nltk.org/)
- [Gensim](https://radimrehurek.com/gensim/index.html)
- [Python Speech Features](https://python-speech-features.readthedocs.io/en/latest/#)
- [Scipy wavfile.read](https://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.io.wavfile.read.html)
- [CUM Sphinx Project](https://cmusphinx.github.io/)
- [CUM ARPAnet Pronouncing Dictionary](http://www.speech.cs.cmu.edu/cgi-bin/cmudict)
- [CUM Lexicon](http://svn.code.sf.net/p/cmusphinx/code/trunk/cmudict/sphinxdict/cmudict_SPHINX_40)
- [Sonic Visualizer](https://www.sonicvisualiser.org/)
- [PySoundFile](https://pysoundfile.readthedocs.io/en/latest/)
- [warp-CTC](https://github.com/baidu-research/warp-ctc)


## Usecases
- https://engineering.fb.com/2016/10/25/ml-applications/building-an-efficient-neural-language-model-over-a-billion-words/
- https://aws.amazon.com/lex/faqs/
- http://www.cs.toronto.edu/~graves/handwriting.cgi?text=My+name+is+Luka&style=&bias=0.15&samples=3
- https://www.youtube.com/watch?v=0FW99AQmMc8
- https://openai.com/blog/dota-2/
- https://github.com/pannous/tensorflow-speech-recognition
- https://github.com/buriburisuri/speech-to-text-wavenet
- https://github.com/mozilla/DeepSpeech
- https://github.com/baidu-research/ba-dls-deepspeech


## Data
- http://ai.stanford.edu/~amaas/data/sentiment/
- https://cocodataset.org/
- [Stats Machine Translation](http://www.statmt.org/)
- https://www.ldc.upenn.edu/
- https://deepai.org/dataset/timit
- http://www.openslr.org/12/
- https://www.tensorflow.org/datasets/catalog/librispeech


## Blogs
- http://jalammar.github.io/
- http://web.science.mq.edu.au/~cassidy/


# Part 1 - Introduction

## Lesson 3 - Intro to NLP

1. NLP is a difficult task because natrual languages are not as structured as mathematical or logical languages.
2. Computers can complete such tasks to a certain level.
    1. process words & phrases
        - keywords
        - part of speech
        - named identities
        - dates & quantities
    2. parse sentences
        - statements
        - questions
        - instructions
    3. analyze Documents
        - frequent & rare words
        - tone & sentiment
        - documents clustering
3. Different contexts give same sentences different meanings, which is difficult for computers to understand.
4. NLP pipelines
    1. **text processing**: obtain & clean plain texts
    2. **feature extraction**: convert texts to numerics
    3. **modeling**: obtain predictions

## Lesson 4 - Text Processing

1. Clean raw data to obtain plain text (e.g. remove html tags).
2. normalization
    - case normalization
    - puncuation removal
    - tokenization: splitting text into tokens (words)
    - stop words removal
    - part of speech tagging
    - named indentity recognition
    - stemming & lemmatization: reduce words to its root
        - stemming: simple, brute rules like removing "ing"
        - lemmatization: using a dictionary to map
  
## Lesson 5 - Spam Classifier with Naive Bayes

1. Bayes Theorem
2. Naive Bayes Algorithm
    1. Assuming B1, B2, etc. are independent, then P(A | B1, B2) is proportional to PP(A) = P(B1|A)\*P(B2|A)\*P(A).
    2. Compute PP(A) and PP(not A)
    3. Normalize PP(A) & PP(not A) so that P(A)+P(not A)=1

## Lesson 6 - Part of Speech Tagging with the Hidden Markov Model (HMM)

1. part of speech (PoS): noun, verb, model verb, adjective, adverb, etc.
2. look up table
    - row: word
    - column: PoS
    - value: frequency in training data
3. bigram look up table
    - row: words bigram
    - column: PoS bigram
    - value: frequency in training data
4. problem of bigram look up table: bigram missing in training data
5. Hidden Markov Model (HMM)
    1. z = (A, B) is a HMM
    2. A: emission probability distribution, i.e. P[Y(t) | X(t)]
    3. B: state transition probability distribution, i.e. P[X(t) | X(t-1)], P[X(0)]
    4. At each time t, X(t) is the hidden state, Y(t) is an observation
    5. likelihood estimation: given z=(A,B) and a set of observation Y, determine P[Y|z]
        - forward algorithm
    6. hidden state decoding: given z={A,B} and a set of observation Y, determine Q, the most likely sequence of hidden states
        - Viterbi algorithm: for each hidden state, find the path that ends with it with highest probability.
    7. Parameter learning: given a model topography (set of states and connections) and a set of observations Y, learning the transition probability A and emission probability B
6. HMM used in PoS tagging
    1. hidden state: part of speech
    2. observations: terms (words)
    3. emission probability: P(term | PoS)
    4. transition probability: P(next PoS | current PoS). Starts and ends of sentences are also considered as PoS.
    5. HMM: combining transition probability among hidden states (PoS) and emission probability between hidden states and observations (terms)
    6. path: connected hidden states with transition probability on edges and emission probability on vertices
    7. estimation: choose the tagging with highest probability along the path

# Part 2 Computing with Natural Language

## Lesson 1 - Feature Extraction and Embeddings

### Bag of Words

- corpus: collection of documents
- vocabulary: collection of unique words in corpus
- document-term matrix
    - columns: terms in vocabulary in some order. Each column is a term
    - rows: each document is a row
    - values: term frequency in the document, or P(t|d) - probability of term occurence in each document
- compare documents (rows)
    - dot product between each row = a . b: a measure of similarity between documents with a flaw: since only overlapping terms contribute to the calculation, both similar and distinct pairs of documents may have same dot product.
    - cosine similarity = a . b / ||a||.||b||: cosine of the angle between the two vectors (rows)

### TF-IDF
- purpose: to assign weights to different term in each documents
- term frequency (TF) tf(t,d) = count(t,d) / |d|
- inverse document frequency (IDF) idf(t,D) = log(|D|+1 / {count of doc containing t})

### One Hot Encoding
- transform each term into a vector with only one element as 1, all others 0
- disadvantage: create too many dimensions since there are a lot of terms

### Embedding
- Only the ith row will be used in the weight matrix will be used when multiplied by the one-hot encoded vector prepresenting ith word. The weight matrix will not be trained in the same way as in a typical NN, but is treatied as a lookup table.

### Word2Vec
- continuous bag of words (CBoW): use neighboring terms to predict the middle term
- continuous skip-gram: use middle terms to predict neighboring terms
    1. Convert any term into an one-hot encoded vector
    2. Use the vector to train a neural network to predict neighboring terms
- Vector size is independent of the vocabulary.
- Train once, stored in a lookup table

### Glove
- co-occurence matrix: P(i|j) where term i co-occured (adjacent or close) with j
- Factorize the co-occurence matrix into a context matrix (terms as context) and a target matrix (target term)

### t-SNE
- t distributed stochastic neighbor embedding
- a way to visualize high dimentional space

## Lesson 2 - Topic Modeling

- bag of words: P(t|d) - probability of term occurence in each document
- Latent Variable: add a topic (z) layer between document and term: P(t|z) and P(z|d)
- Matices: the matrix of P(t|d) equals the multiplication of matix P(t|z) and P(z|d)

### Latent Dirichlet Allocation
- n topics, m terms
- Assumptions
    - P(t|z) follows a Dirichlet distribution with n parameters
    - P(z|d) follows a Dirichlet distribution with m parameters
- Algorithm
    - Initialize distributions of P(t|z) and P(z|d).
    - For each document in the training set, calculate the topic distribution.
    - Simulate a topic sequence of length l from P(z|d) for each document. l is a parameter with a Poisson distribution.
    - For each topic in a simulated sequence, simulate a term from P(t|z).
    - As a result, for each document, a fake document was simulated.
    - Calculate the error of the similarity between fake documents and true documents.
    - Adjust the parameters to move towards maximum similarity (how?)

## Lesson 3 - Sentiment Analysis

Classification between good and bad by:

- classification algorithms
- RNN & LSTM to consider order of words

## Lesson 4 - Sequence to Sequence

- To build a translation machine or a chatbot, one needs a RNN that use a sequence as the input, and output another sequence (at the same time or after a time period of delay).

- Architectures: input -> encoder -> context(state) vector -> decoder -> output where encoder and decoder are two different RNNs

- encoder: every word input into encoder and a vector was generated; For next iteration, next word together with previous vector was input into encoder and a new vector was generated. When all words exhausted in a sequence (sentence), the final vector (as context) was input into decoder.

## Lesson 5 - Attention used in Sequence to Sequence

- Encoder: the encoder in StoS only outputs one context vector; while attention version outputs all vectors corresponding to all words.

- Attention Decoders were able to use all vectors from encoders to generate output, and can adjust the order of sequence as needed.

- A score is assigned to each vector and all scores are normalized using softmax function. A final vector is the weighted average of all vector with normalized vector as weights.

- Choices of scoring functions: additive (Bahdanau) attention v.s. multiplicative (Luong) attention

- multiplicative scoring function
    - dot product: dot product of decoder hidden state and each encoder hidden state (only applicable when both are of the same dimension)
    - general: add a weight matrix between the product of hidden states, which allows difference between the dimensions of encoder and decoder hidden space
    - concat: concatenate encoder and decoder vectors and input them into a single layer feed forward NN and output scores

- Transformer
    - encoder
        - feed-forward layer
        - self-attention layer
    - decoder
        - feed-forward layer
        - encoder-decoder attention
        - self-attention layer

# Part 3 - Communicating with Natrual Language

## Lesson 1 - Intro to VUI (Voice User Interface)

- VUI components: speech to text (speech recognition), text to text, text to speech
- speech to text components: acoustic model, language model, accent model

## Lesson 3 - Speech Recognition

- ASR: automatic speech recognition
- acoustic model: input sound, output phonetic representation
- language model: input phonetic representation, output text

### Whole process
- traditional: speech -> sound signal -> [FFT] -> spectrogram -> [cepstral + mel analysis] -> MFCC features -> [acoustic model(HMM)] -> phonemes -> [lexical decoding] -> words -> [language model(n-gram)]-> text
- deep learning: replace MFCC with CNN, HMM with RNN, lexicon with CTC, n-gram with NLM

### challenges in ASR: 
- noise, 
- variability in: pitch, volume, word speed
- ambiguities that requires language knowledge to distiguish:
    - words that sounds alike
    - word boundaries
    - spelling
    - context
- spoken v.s. written languages

### signal analysis:
- amplitude: how loud the voice is
- signal: sum of component frequencies
- FFT (Fast Fourier Transform) Algorithm: decompose sound signal into component frequences in the form of spectrogram
- spectrogram: 
    - y axis: frequency
    - x axis: time
    - intensity of shades: amplitude

### feature extraction
- mel scale: what sound pitches can human distingush?
- human voice mechanism: source/filter model
    - source is unique to an individual
    - filter is the articulation all use when speaking
    - cepstral analysis: separate source and filter
    - goal: remove source, keep filter
- cepstral analysis + mel analysis -> 12~13 MFCC features (or up to 39 features with optional deltas)
- MFCC feature extraction
    - reduce dimentionality
    - reduce noise

### phonetics
- phonectics is the study of sounds in human languages
- phonemes: unique sound units in a certain language (39~44 in US English)
- grapheme: unique units in a written language (26 letters + space in English)
- arpabet: a US English phonemes set developed in 1971
- speech -> features -> acoustic model -> phonemes -> words -> text
- lexical decoding: the process converting phonemes into words

### acoustic model
- DTW (dynamic time warpping): calculate similarities between 2 signal
- HMM (hidden markov model)

### language model
- input: probability distribution of possible words
- output: most likely sequence of words
- Considering all possible combinations of all words are expensive. Only 3-4 previous words matters in practice.
- n-gram model: estimate proability of next words given previous n words

### CTC (Connectionist Temporal Classification)

# Extracurricular

## Feedforward Neural Network
- output Y = F(x, W)
- static mapping: fixing x and W, Y is fixed.
- training
    - feedforward: given x and W, calculate predicted output and error
        - for each layer h(p) = phi(h(p-1).W(p-1))
        - bias input: 1 as a constant input
    - backpropagation: update W to minimize the error
        - for each group of weights W[t] = W[t-1] + alpha * (- gradient of error w.r.t W)
        - for weight other than the last layer weights, use chain rule to compute gradient
- activation function phi: allow nonlinear relationship between inputs and outputs
    - hyperbolic tangent function: [-1,1]
    - sigmoid function: [0,1]
    - ReLU function: [0,1]
- regularization: dropout
- mini batch training: updating the weights every N steps by using the means of all N deltas

## RNN

- Elman Network (Simple RNN): include output of hidden layer from previous time point as input neurons

- backpropagation through time (BPTT)
    - update W_s - the weight matrix between hidden states: accumulative derivatives
    - update W_x - the weight between input and states: accumulative derivatives

## LSTM

- LTM: long term memory

- STM: short term memory

- E: event/observation

- learn gate
    - input: STM, E
    - output: N * i where N = tanh(W(STM, E)), i = sigmoid(W(STM, E))

- forget gate
    - input: LTM, STM, E
    - output: f * LTM where f = sigmoid(W(STM, E))

- remember gate
    - input: outputs of learn & forget gate
    - output: new LTM = summation of outputs from learn & forget gate

- use gate
    - input outputs from learn & forget gate
    - output: new STM = U * V where U = tanh(W(out of forget gate)) and V = sigmoid(W(output of learn gate, E))
    