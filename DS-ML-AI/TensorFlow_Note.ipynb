{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a658758e-1720-4659-9706-cf415aa157fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-11 09:56:24.086950: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e98e6e-a36d-4877-9b4e-b0e8345f9410",
   "metadata": {},
   "source": [
    "# Tensors & Other Data Structures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a23c0ebe-7e3f-46c3-b6ac-a9f0e27d572d",
   "metadata": {},
   "source": [
    "## Create a Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b6c09b32-5287-4ebd-a2d1-fe11d675924e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-11 09:56:29.818420: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2023-01-11 09:56:29.869848: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1\n",
      "2023-01-11 09:56:30.010815: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-01-11 09:56:30.028945: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \n",
      "pciBusID: 0000:01:00.0 name: NVIDIA GeForce GTX 970 computeCapability: 5.2\n",
      "coreClock: 1.253GHz coreCount: 13 deviceMemorySize: 3.94GiB deviceMemoryBandwidth: 208.91GiB/s\n",
      "2023-01-11 09:56:30.028967: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
      "2023-01-11 09:56:30.111308: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11\n",
      "2023-01-11 09:56:30.111365: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11\n",
      "2023-01-11 09:56:30.183048: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\n",
      "2023-01-11 09:56:30.247643: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\n",
      "2023-01-11 09:56:30.415625: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\n",
      "2023-01-11 09:56:30.443755: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11\n",
      "2023-01-11 09:56:30.451191: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8\n",
      "2023-01-11 09:56:30.451289: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-01-11 09:56:30.451599: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-01-11 09:56:30.459397: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\n",
      "2023-01-11 09:56:30.477608: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2023-01-11 09:56:30.477694: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-01-11 09:56:30.477948: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \n",
      "pciBusID: 0000:01:00.0 name: NVIDIA GeForce GTX 970 computeCapability: 5.2\n",
      "coreClock: 1.253GHz coreCount: 13 deviceMemorySize: 3.94GiB deviceMemoryBandwidth: 208.91GiB/s\n",
      "2023-01-11 09:56:30.477963: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
      "2023-01-11 09:56:30.477978: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11\n",
      "2023-01-11 09:56:30.477988: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11\n",
      "2023-01-11 09:56:30.477998: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\n",
      "2023-01-11 09:56:30.478007: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\n",
      "2023-01-11 09:56:30.478017: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\n",
      "2023-01-11 09:56:30.478026: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11\n",
      "2023-01-11 09:56:30.478035: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8\n",
      "2023-01-11 09:56:30.478080: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-01-11 09:56:30.478354: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-01-11 09:56:30.478636: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\n",
      "2023-01-11 09:56:30.490004: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
      "2023-01-11 09:56:33.082013: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2023-01-11 09:56:33.082039: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0 \n",
      "2023-01-11 09:56:33.082045: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N \n",
      "2023-01-11 09:56:33.093862: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-01-11 09:56:33.094032: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-01-11 09:56:33.094174: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-01-11 09:56:33.094292: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 3506 MB memory) -> physical GPU (device: 0, name: NVIDIA GeForce GTX 970, pci bus id: 0000:01:00.0, compute capability: 5.2)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 2, 5), dtype=int32, numpy=\n",
       "array([[[4, 4, 4, 4, 4],\n",
       "        [4, 4, 4, 4, 4]],\n",
       "\n",
       "       [[4, 4, 4, 4, 4],\n",
       "        [4, 4, 4, 4, 4]],\n",
       "\n",
       "       [[4, 4, 4, 4, 4],\n",
       "        [4, 4, 4, 4, 4]]], dtype=int32)>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a constant tensor\n",
    "\n",
    "const_tensor = tf.constant(value=4, dtype=None, shape=(3,2,5), name=\"const_tensor\")\n",
    "const_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "695fd5aa-7ebf-431d-a460-f93a56bda586",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[[0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0.]]], shape=(3, 2, 5), dtype=float32)\n",
      "\n",
      "tf.Tensor(\n",
      "[[[1. 1. 1. 1. 1.]\n",
      "  [1. 1. 1. 1. 1.]]\n",
      "\n",
      " [[1. 1. 1. 1. 1.]\n",
      "  [1. 1. 1. 1. 1.]]\n",
      "\n",
      " [[1. 1. 1. 1. 1.]\n",
      "  [1. 1. 1. 1. 1.]]], shape=(3, 2, 5), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# Create a all zero (one) constant tensor\n",
    "# By default, both use tf.float32 as the data type\n",
    "zeros_tensor = tf.zeros(shape=(3,2,5))\n",
    "ones_tensor = tf.ones(shape=(3,2,5))\n",
    "\n",
    "print(zeros_tensor)\n",
    "print()\n",
    "print(ones_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9797e06a-0b5a-428e-b733-e628567a6355",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 2, 3), dtype=float32, numpy=\n",
       "array([[[  2.,   3.,   4.],\n",
       "        [ 23.,  34.,   1.]],\n",
       "\n",
       "       [[  6.,  23.,  12.],\n",
       "        [ 44.,  73., 121.]],\n",
       "\n",
       "       [[  5.,   9.,   0.],\n",
       "        [ 12., -34.,  -1.]]], dtype=float32)>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a Tensor from Numpy or Python objects\n",
    "\n",
    "np_array = np.array([[[2.,3.,4.],\n",
    "                      [23.,34.,1.,]],\n",
    "                     [[6.,23.,12.],\n",
    "                      [44.,73.,121.,]],\n",
    "                     [[5.,9.,0.],\n",
    "                      [12.,-34.,-1.,]],\n",
    "                    ])\n",
    "\n",
    "t_np = tf.convert_to_tensor(np_array, dtype=tf.float32)\n",
    "t_np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "986b9ae2-1851-4572-a25f-5b368b14a8d0",
   "metadata": {},
   "source": [
    "## Tensor Indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0dd7236d-1a16-49af-85e1-615e194ba474",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[[  3.   4.]\n",
      "  [ 34.   1.]]\n",
      "\n",
      " [[ 23.  12.]\n",
      "  [ 73. 121.]]\n",
      "\n",
      " [[  9.   0.]\n",
      "  [-34.  -1.]]], shape=(3, 2, 2), dtype=float32)\n",
      "\n",
      "tf.Tensor(\n",
      "[[ 23.  34.   1.]\n",
      " [ 44.  73. 121.]\n",
      " [ 12. -34.  -1.]], shape=(3, 3), dtype=float32)\n",
      "\n",
      "tf.Tensor(\n",
      "[[  3.  34.]\n",
      " [ 23.  73.]\n",
      " [  9. -34.]], shape=(3, 2), dtype=float32)\n",
      "\n",
      "tf.Tensor(\n",
      "[[[  3.]\n",
      "  [ 34.]]\n",
      "\n",
      " [[ 23.]\n",
      "  [ 73.]]\n",
      "\n",
      " [[  9.]\n",
      "  [-34.]]], shape=(3, 2, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# Numpy style indexing\n",
    "# : means up to certain index on a dimension\n",
    "# ... (Ellipsis) means all dimensions that are omitted\n",
    "print(t_np[:, :, 1:])\n",
    "print()\n",
    "print(t_np[..., 1, :])\n",
    "print()\n",
    "print(t_np[..., 1])\n",
    "print()\n",
    "# add a newaxis to keep the dimensions of the origial tensor\n",
    "print(t_np[..., 1, tf.newaxis])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d9bd8fe-364c-4979-a8f6-97360c6fd4bf",
   "metadata": {},
   "source": [
    "## Math Operations\n",
    "\n",
    "- Usually 3 ways to call\n",
    "    - Python Operator: `+`, `-`, `*`, `/`, `//`, `**`, `%`, `@`(matrix multiplication)\n",
    "    - `tf` API aliases: `tf.add()`, `tf.transpose()` (not complete, e.g., there is no alias for `tf.math.log`)\n",
    "    - `tf.math` APT: `tf.math.log`, `tf.math.matmul()`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7f84928-0b1e-40ba-8b96-ba031e614619",
   "metadata": {},
   "source": [
    "## Tensors are Immutable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0cfeebc5-0a3b-4a6b-b6b9-9ff4e6c78a1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "140688835708880\n",
      "140688888117200\n",
      "140688888117376\n"
     ]
    }
   ],
   "source": [
    "# Tensors are immutable\n",
    "# Indexing and operations create a new tensor\n",
    "\n",
    "print(id(t_np))\n",
    "print(id(t_np[:,1,:]))\n",
    "print(id(t_np + 100.))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6548cf32-95d0-4e11-960b-7b6927fb5aca",
   "metadata": {},
   "source": [
    "## Data Types"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ddad040-0228-45e4-8acb-284dbdf24ab3",
   "metadata": {},
   "source": [
    "### Data Type Conversion\n",
    "- TensorFlow does not automatically convert data types during operations.\n",
    "- An exeption was raised if the data types are not compatible with the operation.\n",
    "- `cast` can be used to convert data types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2ab60329-147e-4a70-bd9b-3b459c4aa376",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A tensor of type float64 cannot be added with one of type float32\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=60.0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1 = tf.constant(40, dtype=tf.float64)\n",
    "t2 = tf.constant(20, dtype=tf.float32)\n",
    "\n",
    "try:\n",
    "    t1 + t2\n",
    "except tf.errors.InvalidArgumentError:\n",
    "    print(\"A tensor of type float64 cannot be added with one of type float32\")\n",
    "    \n",
    "tf.add(t2, tf.cast(t1, tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "585e2f73-20ec-406f-87ef-4cb250e6fbea",
   "metadata": {},
   "source": [
    "## Data Structures other than Tensor\n",
    "\n",
    "- Variables\n",
    "    - Since `tf.Tensor` is immutable and cannot be used as weight matrices, `tf.Variable` is needed.\n",
    "    - A variable object can be updated using `assign()` or `scatter_update()`.\n",
    "- `tf.SparseTensor`, opertaion API: `tf.sparse`\n",
    "- `tf.TensorArray`: list of tensors with same shape and data types.\n",
    "- `tf.RaggedTensor`: Static list of lists of tensors with the same shape and data types. Operation API: `tf.ragged`.\n",
    "- String tensors: tensor of data type `tf.string`\n",
    "- Sets: a vector in a tensor's last axis. `tf.sets`\n",
    "- Queues: tensor storage, batching and shuffling: `tf.queue`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcfcad41-5244-4007-bbfe-f31005f5d4e0",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Customizations\n",
    "- One can customize a lot of compotent of a deep learning models\n",
    "    - Loss fuctions\n",
    "    - Activation functions\n",
    "    - Initializers\n",
    "    - Regularizers\n",
    "    - Constraints\n",
    "- 2 ways to implement the customizations\n",
    "    - Simple way when no hyperparameters need to be saved: write a Python function to do it.\n",
    "        - Make the function available to the runtime when loading the model\n",
    "    - Full way when hyperparamters need to be saved: write a Python class to do it.\n",
    "        - Make sure subclassing the proper TensorFlow class.\n",
    "        - Make the class available to the runtime when loading the model\n",
    "        - Map the class name with the actual class in `load_model`.\n",
    "- Try to always use TensorFlow operations to take advantage of the GPU computing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8851aae7-e43a-486c-8341-bbdeb7025557",
   "metadata": {},
   "source": [
    "## Custom Loss Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e52ea222-b6cc-465a-8aac-ef085f8b8d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple way (functional) to implement Huber loss\n",
    "\n",
    "def huber_fn(y_true, y_pred):\n",
    "    error = y_true - y_pred\n",
    "    is_small_error = tf.abs(error) < 1\n",
    "    squared_loss = tf.square(error) / 2\n",
    "    linear_loss = tf.abd(error) - 0.5\n",
    "    return tf.where(is_small_error, square_loss, linear_loss)\n",
    "\n",
    "# # Then to use it\n",
    "# model.compile(loss=huber_fn, optimizer=\"nadam\")\n",
    "# model.fit(X_train, y_train, ...)\n",
    "\n",
    "\n",
    "# If hyperparameters needs to be saved, then use subclass\n",
    "\n",
    "class HuberLoss(tf.keras.losses.Loss):\n",
    "    \n",
    "    def __init__(self, threshold=1.0, **kwargs):\n",
    "        self.threshold = threshold\n",
    "        super().__init__(**kwargs)\n",
    "        \n",
    "    def call(self, y_true, y_pred):\n",
    "        error = y_true - y_pred\n",
    "        is_small_error = tf.abs(error) < self.threshold\n",
    "        squared_loss = tf.square(error) / 2\n",
    "        linear_loss = self.threshold * tf.abd(error) - self.threshold**2 / 2\n",
    "        return tf.where(is_small_error, square_loss, linear_loss)\n",
    "    \n",
    "    def get_config(self):\n",
    "        \"\"\"\n",
    "        A get_config() method is required for a instance to use save() method and tf.keras.model.load_model()\n",
    "        Otherwise only save_weights() and load_weights() can be used.\n",
    "        \"\"\"\n",
    "        base_config = super().get_config()\n",
    "        return {**base_config, \"threshold\": self.threshold}\n",
    "    \n",
    "# # Use the above class\n",
    "# model.compile(loss=HuberLoss(2.), optimizer=\"nadam\")\n",
    "\n",
    "# # When model is loaded\n",
    "# model = keras.models.load_model(\"my_model_with_a_custom_loss_class.h5\", custom_objects={\"HuberLoss\": HuberLoss})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1574fa42-9b49-43ed-9b73-5b47de812272",
   "metadata": {},
   "source": [
    "## Custom Layer Components: Initializers, Regularizers, and Constraints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "90cc5798-3051-482f-aa6d-45ee356ad19b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functional approach, no hyperparameter is needed\n",
    "\n",
    "# Equivalent to keras.initializers.glorot_normal()\n",
    "def my_glorot_initializer(shape, dtype=tf.float32):\n",
    "    stddev = tf.sqrt(2. / (shape[0] + shape[1]))\n",
    "    return tf.random.normal(shape, stddev=stddev, dtype=dtype)\n",
    "\n",
    "# Equivalent to keras.regularizers.l1(0.01)\n",
    "def my_l1_regularizer(weights):\n",
    "    return tf.reduce_sum(tf.abs(0.01 * weights))\n",
    "\n",
    "# Equivalent to keras.constraints.nonneg() or tf.nn.relu()\n",
    "def my_positive_weights(weights):\n",
    "    return tf.where(weights < 0., tf.zeros_like(weights), weights)\n",
    "\n",
    "\n",
    "# If writing a class to include hyperparameters\n",
    "# An example of regularizer, others are similar\n",
    "class MyL1Regularizer(tf.keras.regularizers.Regularizer):\n",
    "    \n",
    "    def __init__(self, factor):\n",
    "        self.factor = factor\n",
    "        \n",
    "    def __call__(self, weights):\n",
    "        return tf.reduce_sum(tf.abs(self.factor * weights))\n",
    "    \n",
    "    def get_config(self):\n",
    "        return {\"factor\": self.factor}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afc36c91-073b-4777-920d-8645f7ca02db",
   "metadata": {},
   "source": [
    "## Custom Activation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7b34fa29-aa21-4395-9025-4b0f2a3801c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Equivalent to keras.activations.softplus() or tf.nn.softplush()\n",
    "def my_softplus(z):\n",
    "    return tf.math.log(tf.exp(z) + 1.0)\n",
    "\n",
    "# If hyparameters are needed, then create a custom layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18374f0e-ce6d-4a27-b9f4-3d9b6e849bc8",
   "metadata": {},
   "source": [
    "## Custom Metrics\n",
    "- Streaming Metrics (Stateful Metrics) keep track of accumulative metric values and the total instance counts. They are gradually updated for each batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5c6258fb-61f5-4d64-9634-39fae405806f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of custom HuberMetric\n",
    "\n",
    "def create_huber(threshold=1.0):\n",
    "    # A function that generate a huber_fn with a certain value of threshold\n",
    "    \n",
    "    def huber_fn(y_true, y_pred):\n",
    "        error = y_true - y_pred\n",
    "        is_small_error = tf.abs(error) < threshold\n",
    "        square_loss = tf.square(error) / 2\n",
    "        linear_loss = threshold * tf.abs(error) - threshold**2 / 2\n",
    "        return tf.where(is_small_error, squared_loss, linear_loss)\n",
    "    return huber_fn\n",
    "\n",
    "        \n",
    "class HuberMetric(tf.keras.metrics.Metric):\n",
    "    \"Custome Metric based on Huber loss formula\"\n",
    "    \n",
    "    def __init__(self, threshold=1.0, **kwargs):\n",
    "        \n",
    "        super().__init__(**kwargs) # handles parent arguments passed in **kwargs\n",
    "        self.threshold = threshold\n",
    "        \n",
    "        # generate huber with dynamic threshold value\n",
    "        self.huber_fn = create_huber(threshold)\n",
    "        \n",
    "        # add 2 tf.Variables using add_weight\n",
    "        # total is the cumulative metric value\n",
    "        # count is the cumulative instance counts\n",
    "        self.total = self.add_weight(\"total\", initializer=\"zeros\")\n",
    "        self.count = self.add_weight(\"count\", initializer=\"zeros\")\n",
    "        \n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        \"Used when the instance of this class is used as a function to update the total and count values\"\n",
    "        metric = self.huber_fn(y_true, y_pred)\n",
    "        self.total.assign_add(tf.reduce_sum(metric))\n",
    "        self.count.assign_add(tf.cast(tf.size(y_true), tf.float32))\n",
    "        \n",
    "    def result(self):\n",
    "        return self.total / self.count\n",
    "    \n",
    "    def get_config(self):\n",
    "        \"\"\"\n",
    "        A get_config() method is required for a instance to use save() method and tf.keras.model.load_model()\n",
    "        Otherwise only save_weights() and load_weights() can be used.\n",
    "        \"\"\"\n",
    "        base_config = super().get_config()\n",
    "        return {**base_config, \"threshold\": self.threshold}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d056de7-0975-438a-a3f1-c3757e0f8723",
   "metadata": {},
   "source": [
    "## Custom Layers\n",
    "- Layers without trainable variables (weights): create a function and wrap it with `tf.keras.layers.Lambda`\n",
    "- Layers with trainable variables (weights): create a subclass of `tf.keras.layers.Layer`\n",
    "- Layers that behave differently in training and scoring (with a training=True or False option). For example `Dropout` or `BatchNormalization`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c23850dc-cbad-4233-8c18-f0e227f2ca4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A simple custom layer without trainable weights\n",
    "# Equivalent to using activation=tf.exp or activation=keras.activations.exponential or activation=\"exponential\"\n",
    "# Can be used as a output layer when the targets have different scales\n",
    "\n",
    "exponential_layer = tf.keras.layers.Lambda(lambda x: tf.exp(x))\n",
    "\n",
    "\n",
    "# A custom Dense layer\n",
    "class MyDense(tf.keras.layers.Layer):\n",
    "    def __init__(self, units, activation=None, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.unit = units\n",
    "        # Turn keywords into actual activation functions\n",
    "        self.activation = tf.keras.activations.get(activation)\n",
    "        \n",
    "    def build(self, batch_input_shape):\n",
    "        self.kernel = self.add_weight(name=\"kernel\", \n",
    "                                      shape=[batch_input_shape[-1], self.units],  \n",
    "                                      initializer=\"glorot_normal\")\n",
    "        self.bias = self.add_weight(name=\"bias\", shape=[self.units], initializer=\"zeros\")\n",
    "        super().build(batch_input_shape)\n",
    "        \n",
    "    def call(self, X):\n",
    "        return self.activation(X @ self.kernel + self.bias)\n",
    "    \n",
    "    def compute_output_shape(self, batch_input_shape):\n",
    "        return tf.TensorShape(batch_input_shape.as_list()[:-1] + [self.units])\n",
    "    \n",
    "    def get_config(self):\n",
    "        \"\"\"\n",
    "        A get_config() method is required for a instance to use save() method and tf.keras.model.load_model()\n",
    "        Otherwise only save_weights() and load_weights() can be used.\n",
    "        \"\"\"\n",
    "        base_config = super().get_config()\n",
    "        return {**base_config, \"units\": self.units, \"activation\": keras.activations.serialize(self.activation)}\n",
    "    \n",
    "\n",
    "# A custom multilayer that concatenate 2 layers and output 3 layers\n",
    "class MyMultiLayer(tf.keras.layers.Layer):\n",
    "    def call(self, X):\n",
    "        X1, X2 = X\n",
    "        return [X1 + X2, X1 * X2, X1 / X2]\n",
    "    \n",
    "    def compute_output_shape(self, batch_input_shape):\n",
    "        b1, b2 = batch_input_shape\n",
    "        return [b1, b1, b1]\n",
    "\n",
    "    \n",
    "# A custom layer with different behavior in training and scoring\n",
    "# Equivalent to tf.keras.layers.GaussianNoise\n",
    "class MyGuassianNoise(tf.keras.layers.Layer):\n",
    "    def __init__(self, stddev, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.stddev = stddev\n",
    "        \n",
    "    def call(self, X, training=None):\n",
    "        if training:\n",
    "            noise = tf.random.normal(tf.shape(X), stddev=self.stddev)\n",
    "            return X + noise\n",
    "        else:\n",
    "            return X\n",
    "    \n",
    "    def compute_output_shape(self, batch_input_shape):\n",
    "        return batch_input_shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72653d83-be93-4149-af15-f62cd20a06f2",
   "metadata": {},
   "source": [
    "## Custom Models\n",
    "- `tf.keras.Model` is a subclass of `tf.keras.layers.Layer`, \n",
    "    - but with more attributes like `compile()`, `fit()`, `evaluate()`, `predict()`, `get_layers()`, and `save()`.\n",
    "    - `save() method` is supported by `tf.keras.models.load_model()` and `tf.keras.model.clone_model()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d2fcec6c-2ee8-415d-8822-62cf8a3fc8ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# An example of custom model that uses a \"residual block\"\n",
    "\n",
    "\n",
    "class ResidualBlock(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    A residual block is a model structure that\n",
    "        1. consists of n_layers dense layers with n_neurons for each layer\n",
    "        2. appends the input to the output\n",
    "    \"\"\"\n",
    "    def __init__(self, n_layers, n_neurons, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.hidden = [tf.keras.layers.Dense(n_neurons, activation=\"elu\", \n",
    "                                             kernel_initialization=\"he_normal\") \n",
    "                       for _ in range(n_layers)]\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        Z = inputs\n",
    "        for layer in self.hidden:\n",
    "            Z = layer(Z)\n",
    "        return inputs + Z\n",
    "    \n",
    "class ResidualRegressor(tf.keras.Model):\n",
    "    \"\"\"\n",
    "    A model structure that consist of instances of ResidualBlock and other dense layers\n",
    "    \"\"\"\n",
    "    def __init__(self, output_dim, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.hidden1 = tf.keras.layers.Dense(30, activation=\"elu\", kernel_initializer=\"he_normal\")\n",
    "        self.block1 = ResidualBlock(2, 30)\n",
    "        self.block2 = ResidualBlock(2, 30)\n",
    "        self.out = keras.layers.Dense(output_dim)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        Z = self.hidden1(inputs)\n",
    "        for _ in range(1 + 3):\n",
    "            Z = self.block1(Z)\n",
    "        Z = self.block2(Z)\n",
    "        return self.out(Z)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f24ff873-59e3-4102-9d13-a292c368ed1e",
   "metadata": {},
   "source": [
    "## Other Customizations\n",
    "- Create a custom model containing a loss/metric function that use the values generated during the training.\n",
    "- Custom training loop (fit()) method that can use multiple optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3058e62-2540-437d-9b14-0bbcd8d0a818",
   "metadata": {},
   "source": [
    "# TensorFlow Graphs\n",
    "\n",
    "## Functions, Autograph, Tracing\n",
    "\n",
    "...To be continued..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35135aa1-6f5a-4d0f-a4a5-dc6190a2aea8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
