# Chapter 17 - Representation Learning & Generative Learning Using Autoencoders & GANs

## Autoencoder
- The purpose is the generate the output that is similar to the input.
- Both input feature and target are the same.
- The output layer should have the same shape with the output layer.
- If no activation function is used and MSE-like loss function is applied, then it is equivalent to PCA.

### Stacked (Deep) Autoencoder Model Structure
- Symmetrical structure with decreasing number of neurons in the first half and increaseing numbers of neurons in the latter half of layers.
- The purpose of the layers with least neurons in the middle is to prevent overfitting.
- Undercomplete autoencoder: the middle layers are smaller.
- The first half is "encoder", the second half is "decoder".
- Encoder's role is to detect the pattern in the input data. While decoder is to generate the output.

### Use Cases of Autoencoder
- Generate images similar to input images
- Reduce dimensions
    - Pro: can handle large datasets
    - Con: results not as good as t-SNE
    - Recommend: use autoencoders to reduce dimentions first, then use t-SNE
- Unsupervised pretraining
    - Good for the case when there are a lot of unlabeled data and some labeled data.
    - Train an autoencoder (using data itself as label) and then replace the trained upper layers with the supervised ones.
- Semantic Interpolation: given 2 images, generate a image that is semantically "between" the original 2.

### Training Tricks of Autoencoders
- Tying weights
    - The autoencoder must be perfectly symmetrical.
    - Copy the lower half (encoder) kernel weights to the upper half (decoder) kernel weights.
    - Reduce the amount of trainable parameters to half.
- Train outer and inner layers separatedly (greedy layerwise training)
    1. Train outer layers first
    2. Train inner layers using the encoded data by the trained outer layers
    3. Combine the inner and outer layers

### Variantions of Autoencoders
- Convolutional Autoencoders
    - Encoder: a CNN with convolutional layers and pooling layers
    - Decoder: transpose covolutional layers (reverse of convolutional layers)
- Recurrent Autoencoders
    - Encoder: sequence-to-vector RNNs
    - Decoder: vector-to-sequence RNNs, using `keras.layers.RepeatVector` as it's first layer

### Overcomplete Autoencoders
- Instead of forcing the middle layers to have a smaller size then the inputs and outputs (undercomplete), there are other contraints enabling the layers as large or larger than the input (overcomplete).
- Denoising autoencoders: adding noise after the input layer to force the model to learn the pattern, not the inputs themselves.
    - Ways to do it
        - Adding Gaussian noises
        - Adding a dropout layer
- Sparse autoencoders: adding regularization to the cost function forces the model to use less active neurons (sparse)
    - Add a large dense layer with sigmoid activation function + a `ActivityRegularization(l1=1e-3)` layer at the end of the encoder
    - Alternative: using Kullback-Leibler (KL) Divergence Regulizer
        - Calculate the KL divergence between the average activation rate of each neuron (across a batch) and a target sparsity, then sum them up across all neurons and then add to the loss function
- Variational autoencoders
    - A new layer between encoder and decoder: the encoder learns the mean and starndard deviation, and a sample was simulated from the Gaussian distribution with the above parameters, and input into the decoder.
    - Loss function: cross_entropy + latent loss, where latent loss is the KL divergence between the fitted Gaussian distribution & and the actual distribution of the coding

## GAN: Generative Adversarial Networks
- Ian Goodfellow et al., 2014
- Model strucure
    - Generator
        - Input: a random ditribution
        - Output: some data (an image)
    - Discriminator
        - Input: a fake image from the generator or a real one from the training set
        - Output: answer to if the input is real or fake
- Traning process: for each iteration:
    1. Train the discriminator
        - Input: a mix of equal number of real and fake images
        - Labels: 0 for fake and 1 for real
        - Loss: binary cross-entropy
        - Only discriminator's weights are updated in this phase
    2. Train the generator
        1. Concatenate the generator and discriminator into a single model GAN
        2. Label all fake images generated by the generator as 1 (real)
        3. Freeze the weights of the discriminator and train the whole GAN
- Challenges of training GANs
    - model collapse: generator forgets previously well trained status on some classes and moves on, ending not good at generating any.

### More GAN Variants
- Deep Convolutional GAN (DCGAN): Alec Radford et al., 2015
- conditional GAN: GAN trained with image label
- Progressive Growing of GANs: Tero Karras et al., Nvidia, 2018
- StyleGAN: Nvidia, 2018